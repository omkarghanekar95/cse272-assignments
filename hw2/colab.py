# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_MxvfggprxTb_gl1xVhZzyNlt8HoaCN7
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd drive/MyDrive/CSE272/hw2/

import numpy as np, pandas as pd

cell_phones_data = pd.read_json('dataset/reviews_subset.json',lines=True)
#print(cell_phones_data)
print(cell_phones_data.head())
# dataframe.to_csv('reviews.csv', sep=',', index=False)

print(cell_phones_data.dtypes)
print('Minimum rating is: %d' %(cell_phones_data.overall.min()))
print('Maximum rating is: %d' %(cell_phones_data.overall.max()))
print('Number of missing values across columns: \n',cell_phones_data.isnull().sum())

print("Total data ")
print("-"*50)
print("\nTotal no of ratings :",cell_phones_data.shape[0])
print("Total No of Users   :", len(np.unique(cell_phones_data.reviewerID)))
print("Total No of products  :", len(np.unique(cell_phones_data.asin)))

cell_phone =  cell_phones_data[["overall", "reviewerID", "asin"]]
cell_phone.asin = cell_phone.asin.apply(lambda x: x[:-1] if 'X' in x else x)

print(cell_phone)

#Analysis of rating given by the user 

no_of_rated_products_per_user = cell_phone.groupby(by='reviewerID')['overall'].count().sort_values(ascending=False)

print(no_of_rated_products_per_user.head())
print(no_of_rated_products_per_user.shape)
print(no_of_rated_products_per_user.describe())

print('\n No of rated product more than 50 per user : {}\n'.format(sum(no_of_rated_products_per_user >= 5)) )

!pwd
cell_csv_data=pd.read_csv("cell_phone_reviews.csv")
cell_csv_data.head()

cell_csv_data.dtypes

!pip install scikit-surprise
from surprise import KNNWithMeans
from surprise import Dataset
from surprise import accuracy
from surprise import Reader

import os
from surprise.model_selection import train_test_split

#Dropping the Timestamp column

cell_csv_data.drop(['timestamp'], axis=1,inplace=True)

#Reading the dataset
#print(cell_csv_data)
# reader = Reader(line_format="overall reviewerID asin", sep=' ', rating_scale=(1, 5),skip_lines=1)
reader = Reader(rating_scale=(1, 5),skip_lines=1)
#data = Dataset.load_from_df(cell_phone[['overall', 'reviewerID', 'asin']], reader)
data = Dataset.load_from_df(cell_csv_data, reader)


#Splitting the dataset
trainset, testset = train_test_split(data, test_size=0.2,random_state=10)

# Use user_based true/false to switch between user-based or item-based collaborative filtering
algo = KNNWithMeans(k=40, sim_options={'name': 'pearson_baseline', 'user_based': False})
algo.fit(trainset)

# run the trained model against the testset
test_pred = algo.test(testset)

print("Item-based Model : Test Set")
accuracy.rmse(test_pred, verbose=True)
accuracy.mae(test_pred, verbose=True)

# Use user_based true/false to switch between user-based or item-based collaborative filtering
user_basedCF = KNNWithMeans(k=40, sim_options={'name': 'pearson_baseline', 'user_based': True})
user_basedCF.fit(trainset)

# run the trained model against the testset
user_basedCF_test_pred = user_basedCF.test(testset)

print("User-based Model : Test Set")
accuracy.rmse(user_basedCF_test_pred, verbose=True)
accuracy.mae(user_basedCF_test_pred, verbose=True)

from surprise import prediction_algorithms
from surprise import SVD
from surprise import NMF


# min_movie_ratings = 2 #a movie has was rated at least 
# min_user_ratings =  5 #a user rated movies at least

# ratings_flrd_df = ratings_df.groupby("movieId").filter(lambda x: x['movieId'].count() >= min_movie_ratings)
# ratings_flrd_df = ratings_flrd_df.groupby("userId").filter(lambda x: x['userId'].count() >= min_user_ratings)

# print("{0} movies deleted; all movies are now rated at least: {1} times. Old dimensions: {2}; New dimensions: {3}"\
# .format(len(ratings_df.movieId.value_counts()) - len(ratings_flrd_df.movieId.value_counts())\
#         ,min_movie_ratings,ratings_df.shape, ratings_flrd_df.shape ))


algo_SVD = SVD(n_factors = 11)
algo_SVD.fit(trainset)


# Predict ratings for all pairs (i,j) that are NOT in the training set.
SVD_testset = trainset.build_anti_testset()

predictions = algo_SVD.test(SVD_testset)

# subset of the list  predictions
predictions[:5]

from collections import defaultdict
def get_top_n(predictions, userId, n = 10):# movies_df, ratings_df,
    '''Return the top N (default) movieId for a user,.i.e. userID and history for comparisom
    Args:
    Returns: 
  
    '''
    #Peart I.: Surprise docomuntation
    
    #1. First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    #2. Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key = lambda x: x[1], reverse = True)
        top_n[uid] = user_ratings[: n ]

    return top_n

print(get_top_n(predictions, 9638762632))

from google.colab import files
uploaded = files.upload()
df4 = pd.read_json("Cell_Phones_and_Accessories_5.json", lines=True)

